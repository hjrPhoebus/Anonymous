<!DOCTYPE html>
<head>
    <meta charset="utf-8" />
    <title>From Inpainting to Editing: A Self-Bootstrapping Paradigm for Context-Rich Visual Dubbing</title>
    <meta content="From Inpainting to Editing: A Self-Bootstrapping Paradigm for Context-Rich Visual Dubbing" name="description" />
    <meta content="summary" name="twitter:card" />
    <meta content="width=device-width, initial-scale=1" name="viewport" />
    <link href="static/css/template.css" rel="stylesheet" type="text/css" />
    <link href="static/css/my_style.css" rel="stylesheet" type="text/css">


    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    
    <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
    <script type="text/javascript">
        WebFont.load({
            google: {
                families: ["Lato:100,100italic,300,300italic,400,400italic,700,700italic,900,900italic", "Montserrat:100,100italic,200,200italic,300,300italic,400,400italic,500,500italic,600,600italic,700,700italic,800,800italic,900,900italic", "Ubuntu:300,300italic,400,400italic,500,500italic,700,700italic", "Changa One:400,400italic", "Open Sans:300,300italic,400,400italic,600,600italic,700,700italic,800,800italic", "Varela Round:400", "Bungee Shade:regular", "Roboto:300,regular,500"]
            }
        });
    </script>
    <script type="text/javascript">
        ! function (o, c) {
            var n = c.documentElement,
                t = " w-mod-";
            n.className += t + "js", ("ontouchstart" in o || o.DocumentTouch && c instanceof DocumentTouch) && (n.className += t + "touch")
        }(window, document);
    </script>
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <script type="text/javascript" src="static/js/zoom.js"></script>
    <script type="text/javascript" src="static/js/video_comparison.js"></script>

    <script async src="https://www.googletagmanager.com/gtag/js?id=G-MLDP9MKGC8"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-MLDP9MKGC8');
    </script>

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet"
            href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/index.js"></script>
    

</head>

<body>

    <div class="section hero nerf-_v2">
        <div class="container-2 nerf_header_v2 w-container">
            <h1 class="nerf_title_v2">
                <span style="background: linear-gradient(to right,  rgb(18, 194, 233), rgb(196, 113, 237), rgb(246, 79, 89)); -webkit-background-clip: text; -webkit-text-fill-color: transparent;">
                    X-Dub:
                    </span>
                    From Inpainting to Editing - A Self-Bootstrapping Paradigm for Context-Rich Visual Dubbing
            </h1>
        </div>
        <!-- <div class="nerf_subheader_v2">Anonymous Authors</div> -->
        <div class="nerf_subheader_v2">
            <div class="external-link">
                <a class="btn" href="https://X-Dub-Lab.github.io" role="button" target="_blank">
                    <i class="ai ai-arxiv"></i> Paper Under Review </a>
            </div>
        </div>
    </div>

    <div data-anchor="slide1" class="section nerf_section">
        <div class="w-container grey_container">
            <h2 class="grey-heading_nerf_abstract">Abstract</h2>
            <p class="paragraph-3 nerf_text nerf_results_text">
              Audio-driven visual dubbing aims to synchronize the lip movements in a talking-head video with a new audio track. A central challenge is the absence of real-world paired training dataâ€”videos with identical identity and poses but different spoken content.
              Existing methods circumvent this by adopting a self-reconstruction paradigm based on masked inpainting, but this yields an incomplete context that splits the model's focus between lip-sync and hallucinating missing content, leading to lip-sync degradation, visual artifacts, and identity drift.
              In this work, we reframe visual dubbing from an under-specified inpainting task to a well-conditioned video-to-video editing problem, proposing a self-bootstrapping paradigm where a Diffusion Transformer (DiT) model generates its own ideal training data. 
              The paradigm features a generator that creates a lip-altered companion video for each training sample, prioritizing identity and stability.
              A subsequent editor then learns the dubbing using these context-rich paired videos as input, which provide stronger conditions than misaligned static reference frames.
              To further enhance performance, we propose a timestep-adaptive multi-phase training strategy that aligns different diffusion stages with learning specific visual features, including structure, lips, and texture. 
              We also introduce the benchmark for robust evaluation in complex, in-the-wild scenarios.
              Experiments show our method achieves state-of-the-art performance, producing high-fidelity results across challenging scenarios.
            </p>
        </div>
    </div>

</body>
<footer>
    Please read the submitted paper for more technical details.</a>
</footer>

</html>
