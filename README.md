# Anonymous
Audio-driven visual dubbing aims to synchronize the lip movements in a talking-head video with a new audio track. A central challenge is the absence of real-world paired training dataâ€”videos with identical identity and poses but different spoken content. Existing methods circumvent this by adopting a self-reconstruction paradigm based on masked inpainting, but this yields an incomplete context that splits the model's focus between lip-sync and hallucinating missing content, leading to lip-sync degradation, visual artifacts, and identity drift. In this work, we reframe visual dubbing from an under-specified inpainting task to a well-conditioned video-to-video editing problem, proposing a self-bootstrapping paradigm where a Diffusion Transformer (DiT) model generates its own ideal training data. The paradigm features a generator that creates a lip-altered companion video for each training sample, prioritizing identity and stability. A subsequent editor then learns the dubbing using these context-rich paired videos as input, which provide stronger conditions than misaligned static reference frames. To further enhance performance, we propose a timestep-adaptive multi-phase training strategy that aligns different diffusion stages with learning specific visual features, including structure, lips, and texture. We also introduce the benchmark for robust evaluation in complex, in-the-wild scenarios. Experiments show our method achieves state-of-the-art performance, producing high-fidelity results across challenging scenarios.
